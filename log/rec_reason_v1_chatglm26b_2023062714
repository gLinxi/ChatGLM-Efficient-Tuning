[2023-06-27 14:39:26,156] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-06-27 14:39:28,602] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-06-27 14:39:28,780] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda112.so
/home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/apps/anaconda3/envs/chatglm_peft_gzx did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11.2/lib64/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 112
CUDA SETUP: Loading binary /home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda112.so...

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda112.so
/home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/apps/anaconda3/envs/chatglm_peft_gzx did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11.2/lib64/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 112
CUDA SETUP: Loading binary /home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda112.so...
06/27/2023 14:39:30 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
06/27/2023 14:39:30 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
06/27/2023 14:39:30 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
06/27/2023 14:39:30 - INFO - utils.common - Process rank: 1, device: cuda:1, n_gpu: 1
  distributed training: True, 16-bits training: True
06/27/2023 14:39:30 - INFO - utils.common - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=16,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./model/rec_reason_v1_chatglm26b_ckp/runs/Jun27_14-39-30_localhost.localdomain,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=./model/rec_reason_v1_chatglm26b_ckp,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=1,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=./model/rec_reason_v1_chatglm26b_ckp,
save_on_each_node=False,
save_safetensors=False,
save_steps=1000,
save_strategy=steps,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
sortish_sampler=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
06/27/2023 14:39:30 - INFO - utils.common - Loading dataset /home/apps/gzx/LocalDataHub/sft/rec_reason_training_task.json...
06/27/2023 14:39:30 - WARNING - utils.common - Checksum failed: missing SHA-1 hash value in dataset_info.json or too many files.
06/27/2023 14:39:30 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
06/27/2023 14:39:30 - INFO - utils.common - Process rank: 0, device: cuda:0, n_gpu: 1
  distributed training: True, 16-bits training: True
06/27/2023 14:39:30 - INFO - utils.common - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=16,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./model/rec_reason_v1_chatglm26b_ckp/runs/Jun27_14-39-30_localhost.localdomain,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=./model/rec_reason_v1_chatglm26b_ckp,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=1,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=./model/rec_reason_v1_chatglm26b_ckp,
save_on_each_node=False,
save_safetensors=False,
save_steps=1000,
save_strategy=steps,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
sortish_sampler=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
06/27/2023 14:39:30 - INFO - utils.common - Loading dataset /home/apps/gzx/LocalDataHub/sft/rec_reason_training_task.json...
06/27/2023 14:39:30 - WARNING - utils.common - Checksum failed: missing SHA-1 hash value in dataset_info.json or too many files.
06/27/2023 14:39:31 - INFO - datasets.builder - Using custom data configuration default-e03ba5b173cf57e1
06/27/2023 14:39:31 - INFO - datasets.info - Loading Dataset Infos from /home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/datasets/packaged_modules/json
06/27/2023 14:39:31 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
06/27/2023 14:39:31 - INFO - datasets.info - Loading Dataset info from /home/apps/.cache/huggingface/datasets/json/default-e03ba5b173cf57e1/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4
06/27/2023 14:39:31 - WARNING - datasets.builder - Found cached dataset json (/home/apps/.cache/huggingface/datasets/json/default-e03ba5b173cf57e1/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 572.29it/s]
06/27/2023 14:39:31 - WARNING - datasets.builder - Found cached dataset json (/home/apps/.cache/huggingface/datasets/json/default-e03ba5b173cf57e1/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
06/27/2023 14:39:31 - INFO - datasets.info - Loading Dataset info from /home/apps/.cache/huggingface/datasets/json/default-e03ba5b173cf57e1/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 631.77it/s]
[INFO|tokenization_utils_base.py:1821] 2023-06-27 14:39:31,890 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1821] 2023-06-27 14:39:31,890 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1821] 2023-06-27 14:39:31,890 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1821] 2023-06-27 14:39:31,890 >> loading file tokenizer_config.json
[WARNING|configuration_utils.py:549] 2023-06-27 14:39:31,920 >> You are using a model of type chatglm to instantiate a model of type . This is not supported for all configurations of models and can yield errors.
[INFO|configuration_utils.py:667] 2023-06-27 14:39:31,923 >> loading configuration file /home/apps/gzx/LocalModelHub/chatglm2_6b/hf/config.json
[INFO|configuration_utils.py:667] 2023-06-27 14:39:31,924 >> loading configuration file /home/apps/gzx/LocalModelHub/chatglm2_6b/hf/config.json
[WARNING|configuration_utils.py:549] 2023-06-27 14:39:31,924 >> You are using a model of type chatglm to instantiate a model of type . This is not supported for all configurations of models and can yield errors.
[INFO|configuration_utils.py:725] 2023-06-27 14:39:31,925 >> Model config ChatGLMConfig {
  "_name_or_path": "/home/apps/gzx/LocalModelHub/chatglm2_6b/hf",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_query_key_layer_scaling": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "configuration_chatglm.ChatGLMConfig",
    "AutoModel": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "modeling_chatglm.ChatGLMForConditionalGeneration"
  },
  "bias_dropout_fusion": true,
  "eos_token_id": 2,
  "ffn_hidden_size": 13696,
  "fp32_residual_connection": false,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1e-05,
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_layers": 28,
  "original_rope": true,
  "pad_token_id": 2,
  "padded_vocab_size": 65024,
  "post_layer_norm": true,
  "quantization_bit": 0,
  "rmsnorm": true,
  "seq_length": 32768,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.30.2",
  "use_cache": true
}

[INFO|modeling_utils.py:2575] 2023-06-27 14:39:32,006 >> loading weights file /home/apps/gzx/LocalModelHub/chatglm2_6b/hf/pytorch_model.bin.index.json
[INFO|configuration_utils.py:577] 2023-06-27 14:39:32,007 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 2,
  "transformers_version": "4.30.2"
}

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:03<00:19,  3.32s/it]Loading checkpoint shards:  14%|█▍        | 1/7 [00:03<00:20,  3.34s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:05<00:14,  2.88s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:06<00:16,  3.38s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:09<00:12,  3.07s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:09<00:12,  3.25s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:11<00:08,  2.79s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:12<00:08,  2.91s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:15<00:06,  3.10s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:15<00:06,  3.21s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:19<00:03,  3.37s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:19<00:03,  3.34s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:21<00:00,  2.96s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:21<00:00,  3.03s/it]
[INFO|modeling_utils.py:3295] 2023-06-27 14:39:53,320 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.

[INFO|modeling_utils.py:3303] 2023-06-27 14:39:53,320 >> All the weights of ChatGLMForConditionalGeneration were initialized from the model checkpoint at /home/apps/gzx/LocalModelHub/chatglm2_6b/hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use ChatGLMForConditionalGeneration for predictions without further training.
[INFO|modeling_utils.py:2927] 2023-06-27 14:39:53,322 >> Generation config file not found, using a generation config created from the model config.
06/27/2023 14:39:53 - INFO - utils.common - Fine-tuning method: LoRA
Loading checkpoint shards: 100%|██████████| 7/7 [00:21<00:00,  2.92s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:21<00:00,  3.09s/it]
06/27/2023 14:39:53 - INFO - utils.common - Fine-tuning method: LoRA
trainable params: 1949696 || all params: 6245533696 || trainable%: 0.0312
Running tokenizer on dataset:   0%|          | 0/5000 [00:00<?, ? examples/s]06/27/2023 14:40:00 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/apps/.cache/huggingface/datasets/json/default-e03ba5b173cf57e1/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-4ef74f89af67c74a.arrow
Running tokenizer on dataset:  20%|██        | 1000/5000 [00:00<00:02, 1581.32 examples/s]trainable params: 1949696 || all params: 6245533696 || trainable%: 0.0312
Running tokenizer on dataset:  40%|████      | 2000/5000 [00:01<00:01, 1611.44 examples/s]Running tokenizer on dataset:  60%|██████    | 3000/5000 [00:01<00:01, 1621.74 examples/s]Running tokenizer on dataset:  80%|████████  | 4000/5000 [00:02<00:00, 1616.79 examples/s]Running tokenizer on dataset: 100%|██████████| 5000/5000 [00:03<00:00, 1620.29 examples/s]                                                                                          input_ids:
[64790, 64792, 790, 30951, 517, 30910, 30939, 30996, 13, 13, 54761, 31211, 31822, 32065, 53015, 31791, 31744, 49635, 34677, 54619, 32941, 54530, 35195, 34677, 31123, 54556, 54236, 41578, 32941, 54530, 32727, 35195, 34677, 31123, 54724, 55155, 54571, 54236, 34677, 32667, 32911, 54887, 31123, 31696, 32363, 32495, 33652, 48084, 31123, 54952, 54744, 33840, 36410, 30966, 30940, 54550, 54952, 13, 296, 31791, 31744, 49635, 34677, 31211, 55163, 55149, 31007, 36053, 31007, 32645, 54931, 31007, 37035, 31007, 38427, 31007, 32033, 54931, 31007, 55430, 57264, 31007, 54940, 56949, 55835, 31007, 55120, 54635, 54931, 31007, 34277, 31007, 39783, 31007, 57212, 55386, 31007, 34454, 54931, 31007, 55120, 54635, 31007, 33408, 54936, 55202, 31007, 31638, 31007, 54620, 54561, 55145, 31007, 58686, 57440, 57440, 31007, 56635, 56048, 55529, 31007, 32330, 31709, 31007, 36463, 31007, 54940, 55608, 31007, 51219, 31007, 40136, 31123, 32941, 54611, 55296, 56007, 30944, 4678, 30961, 54586, 43680, 38682, 54612, 54530, 35195, 34677, 31211, 43399, 31007, 55120, 54635, 54931, 31007, 55163, 55149, 13, 296, 13, 13, 55437, 31211, 30910, 54236, 34677, 31211, 55120, 54635, 54931, 31007, 55163, 55149, 31155, 32495, 33652, 33088, 55296, 56007, 30944, 4678, 30961, 54586, 43680, 38682, 54612, 32104, 43399, 31201, 55120, 54635, 54931, 54609, 33162, 31123, 31738, 31815, 55163, 55149, 31201, 55120, 54635, 54931, 40024, 32540, 31870, 31123, 34285, 32941, 44762, 53208, 32286, 37373, 54542, 31848, 39255, 54629, 32600, 31155, 2]
inputs:
[Round 1]

问：你的任务是根据我的历史偏好标签与影片的附属标签，来推断我喜欢影片的哪些附属标签，并仅用推断标签组成一段话，要求符合推荐理由的风格，字数尽量不超过30个字
    我的历史偏好标签：益智|亲子|娱乐类|喜剧|冒险|游戏类|童谣|张秉君|早教类|动画|宠物|谭笑|辅导类|早教|英文儿歌|生活|合家欢|缪莹莹|贾晨露|语言能力|玩具|张伟|动画片|友情，影片《飞狗MOCO之我爱我家》的附属标签：奇幻|早教类|益智
    

答： 推断标签：早教类|益智。推荐理由：《飞狗MOCO之我爱我家》拥有奇幻、早教类等元素，对于喜欢益智、早教类内容的观众来说，这部影片一定能给您带来乐趣和知识的双重享受。
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30910, 54236, 34677, 31211, 55120, 54635, 54931, 31007, 55163, 55149, 31155, 32495, 33652, 33088, 55296, 56007, 30944, 4678, 30961, 54586, 43680, 38682, 54612, 32104, 43399, 31201, 55120, 54635, 54931, 54609, 33162, 31123, 31738, 31815, 55163, 55149, 31201, 55120, 54635, 54931, 40024, 32540, 31870, 31123, 34285, 32941, 44762, 53208, 32286, 37373, 54542, 31848, 39255, 54629, 32600, 31155, 2]
labels:
推断标签：早教类|益智。推荐理由：《飞狗MOCO之我爱我家》拥有奇幻、早教类等元素，对于喜欢益智、早教类内容的观众来说，这部影片一定能给您带来乐趣和知识的双重享受。
Running tokenizer on dataset:   0%|          | 0/5000 [00:00<?, ? examples/s]Running tokenizer on dataset:  20%|██        | 1000/5000 [00:00<00:02, 1417.81 examples/s]Running tokenizer on dataset:  40%|████      | 2000/5000 [00:01<00:02, 1465.07 examples/s]Running tokenizer on dataset:  60%|██████    | 3000/5000 [00:02<00:01, 1475.51 examples/s]Running tokenizer on dataset:  80%|████████  | 4000/5000 [00:02<00:00, 1462.41 examples/s]Running tokenizer on dataset: 100%|██████████| 5000/5000 [00:03<00:00, 1464.82 examples/s]                                                                                          input_ids:
[64790, 64792, 790, 30951, 517, 30910, 30939, 30996, 13, 13, 54761, 31211, 31822, 32065, 53015, 31791, 31744, 49635, 34677, 54619, 32941, 54530, 35195, 34677, 31123, 54556, 54236, 41578, 32941, 54530, 32727, 35195, 34677, 31123, 54724, 55155, 54571, 54236, 34677, 32667, 32911, 54887, 31123, 31696, 32363, 32495, 33652, 48084, 31123, 54952, 54744, 33840, 36410, 30966, 30940, 54550, 54952, 13, 296, 31791, 31744, 49635, 34677, 31211, 55163, 55149, 31007, 36053, 31007, 32645, 54931, 31007, 37035, 31007, 38427, 31007, 32033, 54931, 31007, 55430, 57264, 31007, 54940, 56949, 55835, 31007, 55120, 54635, 54931, 31007, 34277, 31007, 39783, 31007, 57212, 55386, 31007, 34454, 54931, 31007, 55120, 54635, 31007, 33408, 54936, 55202, 31007, 31638, 31007, 54620, 54561, 55145, 31007, 58686, 57440, 57440, 31007, 56635, 56048, 55529, 31007, 32330, 31709, 31007, 36463, 31007, 54940, 55608, 31007, 51219, 31007, 40136, 31123, 32941, 54611, 55296, 56007, 30944, 4678, 30961, 54586, 43680, 38682, 54612, 54530, 35195, 34677, 31211, 43399, 31007, 55120, 54635, 54931, 31007, 55163, 55149, 13, 296, 13, 13, 55437, 31211, 30910, 54236, 34677, 31211, 55120, 54635, 54931, 31007, 55163, 55149, 31155, 32495, 33652, 33088, 55296, 56007, 30944, 4678, 30961, 54586, 43680, 38682, 54612, 32104, 43399, 31201, 55120, 54635, 54931, 54609, 33162, 31123, 31738, 31815, 55163, 55149, 31201, 55120, 54635, 54931, 40024, 32540, 31870, 31123, 34285, 32941, 44762, 53208, 32286, 37373, 54542, 31848, 39255, 54629, 32600, 31155, 2]
inputs:
[Round 1]

问：你的任务是根据我的历史偏好标签与影片的附属标签，来推断我喜欢影片的哪些附属标签，并仅用推断标签组成一段话，要求符合推荐理由的风格，字数尽量不超过30个字
    我的历史偏好标签：益智|亲子|娱乐类|喜剧|冒险|游戏类|童谣|张秉君|早教类|动画|宠物|谭笑|辅导类|早教|英文儿歌|生活|合家欢|缪莹莹|贾晨露|语言能力|玩具|张伟|动画片|友情，影片《飞狗MOCO之我爱我家》的附属标签：奇幻|早教类|益智
    

答： 推断标签：早教类|益智。推荐理由：《飞狗MOCO之我爱我家》拥有奇幻、早教类等元素，对于喜欢益智、早教类内容的观众来说，这部影片一定能给您带来乐趣和知识的双重享受。
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30910, 54236, 34677, 31211, 55120, 54635, 54931, 31007, 55163, 55149, 31155, 32495, 33652, 33088, 55296, 56007, 30944, 4678, 30961, 54586, 43680, 38682, 54612, 32104, 43399, 31201, 55120, 54635, 54931, 54609, 33162, 31123, 31738, 31815, 55163, 55149, 31201, 55120, 54635, 54931, 40024, 32540, 31870, 31123, 34285, 32941, 44762, 53208, 32286, 37373, 54542, 31848, 39255, 54629, 32600, 31155, 2]
labels:
推断标签：早教类|益智。推荐理由：《飞狗MOCO之我爱我家》拥有奇幻、早教类等元素，对于喜欢益智、早教类内容的观众来说，这部影片一定能给您带来乐趣和知识的双重享受。
[INFO|trainer.py:1786] 2023-06-27 14:40:17,758 >> ***** Running training *****
[INFO|trainer.py:1787] 2023-06-27 14:40:17,759 >>   Num examples = 5,000
[INFO|trainer.py:1788] 2023-06-27 14:40:17,759 >>   Num Epochs = 3
[INFO|trainer.py:1789] 2023-06-27 14:40:17,759 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:1790] 2023-06-27 14:40:17,759 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1791] 2023-06-27 14:40:17,759 >>   Gradient Accumulation steps = 16
[INFO|trainer.py:1792] 2023-06-27 14:40:17,759 >>   Total optimization steps = 468
[INFO|trainer.py:1793] 2023-06-27 14:40:17,762 >>   Number of trainable parameters = 1,949,696
  0%|          | 0/468 [00:00<?, ?it/s]  0%|          | 1/468 [00:06<48:41,  6.25s/it]06/27/2023 14:40:24 - INFO - torch.nn.parallel.distributed - Reducer buckets have been rebuilt in this iteration.
06/27/2023 14:40:24 - INFO - torch.nn.parallel.distributed - Reducer buckets have been rebuilt in this iteration.
  0%|          | 2/468 [00:11<44:05,  5.68s/it]  1%|          | 3/468 [00:16<41:58,  5.42s/it]  1%|          | 4/468 [00:21<40:54,  5.29s/it]  1%|          | 5/468 [00:26<40:31,  5.25s/it]  1%|▏         | 6/468 [00:32<40:24,  5.25s/it]  1%|▏         | 7/468 [00:37<40:45,  5.30s/it]  2%|▏         | 8/468 [00:42<40:34,  5.29s/it]  2%|▏         | 9/468 [00:48<40:42,  5.32s/it]  2%|▏         | 10/468 [00:53<41:40,  5.46s/it]                                                {'loss': 1.4184, 'learning_rate': 0.0009988738792578126, 'epoch': 0.06}
  2%|▏         | 10/468 [00:53<41:40,  5.46s/it]WARNING:torch.distributed.elastic.agent.server.api:Received 2 death signal, shutting down workers
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 102300 closing signal SIGINT
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 102301 closing signal SIGINT
Traceback (most recent call last):
  File "/home/apps/gzx/ChatGLM_PEFT/ChatGLM-Efficient/./src/train_sft.py", line 105, in <module>
    main()
  File "/home/apps/gzx/ChatGLM_PEFT/ChatGLM-Efficient/./src/train_sft.py", line 73, in main
    train_result = trainer.train()
  File "/home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/transformers/trainer.py", line 1645, in train
    return inner_training_loop(
  File "/home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/transformers/trainer.py", line 1938, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/transformers/trainer.py", line 2770, in training_step
Traceback (most recent call last):
  File "/home/apps/gzx/ChatGLM_PEFT/ChatGLM-Efficient/./src/train_sft.py", line 105, in <module>
    self.accelerator.backward(loss)
  File "/home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/accelerate/accelerator.py", line 1819, in backward
    main()
  File "/home/apps/gzx/ChatGLM_PEFT/ChatGLM-Efficient/./src/train_sft.py", line 73, in main
    train_result = trainer.train()
  File "/home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/transformers/trainer.py", line 1645, in train
    self.scaler.scale(loss).backward(**kwargs)
  File "/home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/torch/_tensor.py", line 487, in backward
    return inner_training_loop(
  File "/home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/transformers/trainer.py", line 1938, in _inner_training_loop
    torch.autograd.backward(
  File "/home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
    tr_loss_step = self.training_step(model, inputs)
  File "/home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/transformers/trainer.py", line 2770, in training_step
    self.accelerator.backward(loss)
  File "/home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/accelerate/accelerator.py", line 1819, in backward
    self.scaler.scale(loss).backward(**kwargs)
  File "/home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
  2%|▏         | 10/468 [00:55<42:28,  5.57s/it]
Traceback (most recent call last):
  File "/home/apps/anaconda3/envs/chatglm_peft_gzx/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 45, in main
    args.func(args)
  File "/home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/accelerate/commands/launch.py", line 932, in launch_command
    multi_gpu_launcher(args)
  File "/home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/accelerate/commands/launch.py", line 627, in multi_gpu_launcher
    distrib_run.run(args)
  File "/home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 241, in launch_agent
    result = agent.run()
  File "/home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 129, in wrapper
    result = f(*args, **kwargs)
  File "/home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 723, in run
    result = self._invoke_run(role)
  File "/home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 864, in _invoke_run
    time.sleep(monitor_interval)
  File "/home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 102238 got signal: 2
[2023-06-27 14:44:45,926] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-06-27 14:44:48,598] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-06-27 14:44:48,603] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda112.so
/home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/apps/anaconda3/envs/chatglm_peft_gzx did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('3928'), PosixPath('//172.22.17.106')}
  warn(msg)
/home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vb=\\Eg'), PosixPath('li#55'), PosixPath('ks=\\E[?1h\\E='), PosixPath('F6=\\E[1;2S'), PosixPath('LP'), PosixPath('kR=\\E[1;2A'), PosixPath('co#238'), PosixPath('ve=\\E[34h\\E[?25h'), PosixPath('ku=\\EOA'), PosixPath('dc=\\E[P'), PosixPath('%e=\\E[5;2~'), PosixPath('ei=\\E[4l'), PosixPath('bs'), PosixPath('ue=\\E[24m'), PosixPath('LE=\\E[%dD'), PosixPath('mb=\\E[5m'), PosixPath('km'), PosixPath('kF=\\E[1;2B'), PosixPath('bt=\\E[Z'), PosixPath('kd=\\EOB'), PosixPath('AX'), PosixPath('F7=\\E[15;2~'), PosixPath('DO=\\E[%dB'), PosixPath('rc=\\E8'), PosixPath('%c=\\E[6;2~'), PosixPath('ce=\\E[K'), PosixPath('up=\\EM'), PosixPath('is=\\E)0'), PosixPath('AL=\\E[%dL'), PosixPath('FA=\\E[19;2~'), PosixPath('kP=\\E[5~'), PosixPath('kr=\\EOC'), PosixPath('kN=\\E[6~'), PosixPath('us=\\E[4m'), PosixPath('G0'), PosixPath('AB=\\E[4%dm'), PosixPath('AF=\\E[3%dm'), PosixPath('UP=\\E[%dA'), PosixPath('ms'), PosixPath('#4=\\E[1;2D'), PosixPath('#2=\\E[1;2H'), PosixPath('vi=\\E[?25l'), PosixPath('ac=\\140\\140aaffggjjkkllmmnnooppqqrrssttuuvvwwxxyyzz{{||}}~~..--++,,hhII00'), PosixPath('kB=\\E[Z'), PosixPath('op=\\E[39;49m'), PosixPath('sr=\\EM'), PosixPath('it#8'), PosixPath('k3=\\EOR'), PosixPath('sc=\\E7'), PosixPath('F2=\\E[24~'), PosixPath('se=\\E[23m'), PosixPath('k6=\\E[17~'), PosixPath('po=\\E[5i'), PosixPath('k2=\\EOQ'), PosixPath('mr=\\E[7m'), PosixPath('cm=\\E[%i%d;%dH'), PosixPath('ct=\\E[3g'), PosixPath('pa#64'), PosixPath('#3=\\E[2;2~'), PosixPath('k;=\\E[21~'), PosixPath('\\\n\t'), PosixPath('rs=\\Ec'), PosixPath('K2=\\EOE'), PosixPath('st=\\EH'), PosixPath('F4=\\E[1;2Q'), PosixPath('k1=\\EOP'), PosixPath('F1=\\E[23~'), PosixPath('F3=\\E[1;2P'), PosixPath('ti=\\E[?1049h'), PosixPath('ta=^I'), PosixPath('kl=\\EOD'), PosixPath('vs=\\E[34l'), PosixPath('le=^H'), PosixPath('Km=\\E[M'), PosixPath('am'), PosixPath('*4=\\E[3;2~'), PosixPath('DL=\\E[%dM'), PosixPath('kI=\\E[2~'), PosixPath('k4=\\EOS'), PosixPath('kh=\\E[1~'), PosixPath('k5=\\E[15~'), PosixPath('pf=\\E[4i'), PosixPath('F5=\\E[1;2R'), PosixPath('bl=^G'), PosixPath('@7=\\E[4~'), PosixPath('cr=^M'), PosixPath('nw=\\EE'), PosixPath('pt'), PosixPath('im=\\E[4h'), PosixPath('Co#8'), PosixPath('*7=\\E[1;2F'), PosixPath('@1=\\E[1~'), PosixPath('IC=\\E[%d@'), PosixPath('kH=\\E[4~'), PosixPath('as=\\E(0'), PosixPath('nd=\\E[C'), PosixPath('k7=\\E[18~'), PosixPath('dl=\\E[M'), PosixPath('cs=\\E[%i%d;%dr'), PosixPath('xv'), PosixPath('ke=\\E[?1l\\E>'), PosixPath('do=^J'), PosixPath('so=\\E[3m'), PosixPath('ho=\\E[H'), PosixPath('te=\\E[?1049l'), PosixPath('al=\\E[L'), PosixPath('F8=\\E[17;2~'), PosixPath('%i=\\E[1;2C'), PosixPath('SC|screen|VT 100/ANSI X3.64 virtual terminal'), PosixPath('DC=\\E[%dP'), PosixPath('xn'), PosixPath('kD=\\E[3~'), PosixPath('k0=\\E[10~'), PosixPath('cd=\\E[J'), PosixPath('RI=\\E[%dC'), PosixPath('md=\\E[1m'), PosixPath('ae=\\E(B'), PosixPath('F9=\\E[18;2~'), PosixPath('k8=\\E[19~'), PosixPath('mi'), PosixPath('kb=\x7f'), PosixPath('cl=\\E[H\\E[J'), PosixPath('k9=\\E[20~'), PosixPath('me=\\E[m')}
  warn(msg)
/home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('Asia/Shanghai')}
  warn(msg)
/home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_g3_1lway/none_4kgs3nha/attempt_0/1/error.json')}
  warn(msg)
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
/home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.
Either way, this might cause trouble in the future:
If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.
  warn(msg)
CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 112
CUDA SETUP: Loading binary /home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda112.so...
bin /home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda112.so
/home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/apps/anaconda3/envs/chatglm_peft_gzx did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('3928'), PosixPath('http'), PosixPath('//172.22.17.106')}
  warn(msg)
/home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('k4=\\EOS'), PosixPath('G0'), PosixPath('AL=\\E[%dL'), PosixPath('k9=\\E[20~'), PosixPath('%e=\\E[5;2~'), PosixPath('mi'), PosixPath('F3=\\E[1;2P'), PosixPath('sr=\\EM'), PosixPath('ti=\\E[?1049h'), PosixPath('rc=\\E8'), PosixPath('co#238'), PosixPath('k3=\\EOR'), PosixPath('DL=\\E[%dM'), PosixPath('kd=\\EOB'), PosixPath('k8=\\E[19~'), PosixPath('DO=\\E[%dB'), PosixPath('F4=\\E[1;2Q'), PosixPath('kF=\\E[1;2B'), PosixPath('ac=\\140\\140aaffggjjkkllmmnnooppqqrrssttuuvvwwxxyyzz{{||}}~~..--++,,hhII00'), PosixPath('kP=\\E[5~'), PosixPath('as=\\E(0'), PosixPath('pt'), PosixPath('st=\\EH'), PosixPath('K2=\\EOE'), PosixPath('LP'), PosixPath('#2=\\E[1;2H'), PosixPath('IC=\\E[%d@'), PosixPath('k7=\\E[18~'), PosixPath('us=\\E[4m'), PosixPath('#3=\\E[2;2~'), PosixPath('nw=\\EE'), PosixPath('pf=\\E[4i'), PosixPath('k5=\\E[15~'), PosixPath('ks=\\E[?1h\\E='), PosixPath('%i=\\E[1;2C'), PosixPath('Co#8'), PosixPath('xn'), PosixPath('F7=\\E[15;2~'), PosixPath('k6=\\E[17~'), PosixPath('F8=\\E[17;2~'), PosixPath('k;=\\E[21~'), PosixPath('F5=\\E[1;2R'), PosixPath('po=\\E[5i'), PosixPath('kB=\\E[Z'), PosixPath('LE=\\E[%dD'), PosixPath('it#8'), PosixPath('FA=\\E[19;2~'), PosixPath('up=\\EM'), PosixPath('kr=\\EOC'), PosixPath('\\\n\t'), PosixPath('AF=\\E[3%dm'), PosixPath('ue=\\E[24m'), PosixPath('ei=\\E[4l'), PosixPath('dl=\\E[M'), PosixPath('kD=\\E[3~'), PosixPath('pa#64'), PosixPath('AB=\\E[4%dm'), PosixPath('rs=\\Ec'), PosixPath('kN=\\E[6~'), PosixPath('se=\\E[23m'), PosixPath('ms'), PosixPath('ce=\\E[K'), PosixPath('@1=\\E[1~'), PosixPath('do=^J'), PosixPath('li#55'), PosixPath('le=^H'), PosixPath('ke=\\E[?1l\\E>'), PosixPath('kh=\\E[1~'), PosixPath('cl=\\E[H\\E[J'), PosixPath('k1=\\EOP'), PosixPath('cm=\\E[%i%d;%dH'), PosixPath('md=\\E[1m'), PosixPath('al=\\E[L'), PosixPath('UP=\\E[%dA'), PosixPath('ae=\\E(B'), PosixPath('kb=\x7f'), PosixPath('is=\\E)0'), PosixPath('so=\\E[3m'), PosixPath('kI=\\E[2~'), PosixPath('nd=\\E[C'), PosixPath('*7=\\E[1;2F'), PosixPath('km'), PosixPath('cd=\\E[J'), PosixPath('F9=\\E[18;2~'), PosixPath('vb=\\Eg'), PosixPath('DC=\\E[%dP'), PosixPath('mb=\\E[5m'), PosixPath('F2=\\E[24~'), PosixPath('@7=\\E[4~'), PosixPath('xv'), PosixPath('ho=\\E[H'), PosixPath('vs=\\E[34l'), PosixPath('*4=\\E[3;2~'), PosixPath('op=\\E[39;49m'), PosixPath('sc=\\E7'), PosixPath('F1=\\E[23~'), PosixPath('kl=\\EOD'), PosixPath('%c=\\E[6;2~'), PosixPath('kH=\\E[4~'), PosixPath('dc=\\E[P'), PosixPath('me=\\E[m'), PosixPath('ku=\\EOA'), PosixPath('ct=\\E[3g'), PosixPath('ta=^I'), PosixPath('k0=\\E[10~'), PosixPath('mr=\\E[7m'), PosixPath('bt=\\E[Z'), PosixPath('am'), PosixPath('RI=\\E[%dC'), PosixPath('#4=\\E[1;2D'), PosixPath('F6=\\E[1;2S'), PosixPath('Km=\\E[M'), PosixPath('k2=\\EOQ'), PosixPath('AX'), PosixPath('cs=\\E[%i%d;%dr'), PosixPath('cr=^M'), PosixPath('bs'), PosixPath('ve=\\E[34h\\E[?25h'), PosixPath('bl=^G'), PosixPath('te=\\E[?1049l'), PosixPath('im=\\E[4h'), PosixPath('kR=\\E[1;2A'), PosixPath('vi=\\E[?25l'), PosixPath('SC|screen|VT 100/ANSI X3.64 virtual terminal')}
  warn(msg)
/home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('Asia/Shanghai')}
  warn(msg)
/home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_g3_1lway/none_4kgs3nha/attempt_0/0/error.json')}
  warn(msg)
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
/home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.
Either way, this might cause trouble in the future:
If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.
  warn(msg)
CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 112
CUDA SETUP: Loading binary /home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda112.so...
06/27/2023 14:44:50 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
06/27/2023 14:44:50 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
06/27/2023 14:44:50 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
06/27/2023 14:44:50 - INFO - utils.common - Process rank: 0, device: cuda:0, n_gpu: 1
  distributed training: True, 16-bits training: True
06/27/2023 14:44:50 - INFO - utils.common - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=16,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./model/rec_reason_v1_chatglm26b_ckp/runs/Jun27_14-44-50_localhost.localdomain,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=./model/rec_reason_v1_chatglm26b_ckp,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=1,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=./model/rec_reason_v1_chatglm26b_ckp,
save_on_each_node=False,
save_safetensors=False,
save_steps=1000,
save_strategy=steps,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
sortish_sampler=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
06/27/2023 14:44:50 - INFO - utils.common - Loading dataset /home/apps/gzx/LocalDataHub/sft/rec_reason_training_task.json...
06/27/2023 14:44:50 - WARNING - utils.common - Checksum failed: missing SHA-1 hash value in dataset_info.json or too many files.
06/27/2023 14:44:50 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
06/27/2023 14:44:50 - INFO - utils.common - Process rank: 1, device: cuda:1, n_gpu: 1
  distributed training: True, 16-bits training: True
06/27/2023 14:44:50 - INFO - utils.common - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=16,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./model/rec_reason_v1_chatglm26b_ckp/runs/Jun27_14-44-50_localhost.localdomain,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=./model/rec_reason_v1_chatglm26b_ckp,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=1,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=./model/rec_reason_v1_chatglm26b_ckp,
save_on_each_node=False,
save_safetensors=False,
save_steps=1000,
save_strategy=steps,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
sortish_sampler=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
06/27/2023 14:44:50 - INFO - utils.common - Loading dataset /home/apps/gzx/LocalDataHub/sft/rec_reason_training_task.json...
06/27/2023 14:44:50 - WARNING - utils.common - Checksum failed: missing SHA-1 hash value in dataset_info.json or too many files.
06/27/2023 14:44:51 - INFO - datasets.builder - Using custom data configuration default-e03ba5b173cf57e1
06/27/2023 14:44:51 - INFO - datasets.info - Loading Dataset Infos from /home/apps/anaconda3/envs/chatglm_peft_gzx/lib/python3.10/site-packages/datasets/packaged_modules/json
06/27/2023 14:44:51 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
06/27/2023 14:44:51 - INFO - datasets.info - Loading Dataset info from /home/apps/.cache/huggingface/datasets/json/default-e03ba5b173cf57e1/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4
06/27/2023 14:44:51 - WARNING - datasets.builder - Found cached dataset json (/home/apps/.cache/huggingface/datasets/json/default-e03ba5b173cf57e1/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
06/27/2023 14:44:51 - INFO - datasets.info - Loading Dataset info from /home/apps/.cache/huggingface/datasets/json/default-e03ba5b173cf57e1/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 573.38it/s]
[INFO|tokenization_utils_base.py:1821] 2023-06-27 14:44:51,693 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1821] 2023-06-27 14:44:51,694 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1821] 2023-06-27 14:44:51,694 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1821] 2023-06-27 14:44:51,694 >> loading file tokenizer_config.json
06/27/2023 14:44:51 - WARNING - datasets.builder - Found cached dataset json (/home/apps/.cache/huggingface/datasets/json/default-e03ba5b173cf57e1/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
  0%|          | 0/1 [00:00<?, ?it/s][INFO|configuration_utils.py:667] 2023-06-27 14:44:51,727 >> loading configuration file /home/apps/gzx/LocalModelHub/chatglm2_6b/hf/config.json
100%|██████████| 1/1 [00:00<00:00, 559.17it/s]
[INFO|configuration_utils.py:667] 2023-06-27 14:44:51,728 >> loading configuration file /home/apps/gzx/LocalModelHub/chatglm2_6b/hf/config.json
[WARNING|configuration_utils.py:549] 2023-06-27 14:44:51,728 >> You are using a model of type chatglm to instantiate a model of type . This is not supported for all configurations of models and can yield errors.
[INFO|configuration_utils.py:725] 2023-06-27 14:44:51,729 >> Model config ChatGLMConfig {
  "_name_or_path": "/home/apps/gzx/LocalModelHub/chatglm2_6b/hf",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_query_key_layer_scaling": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "configuration_chatglm.ChatGLMConfig",
    "AutoModel": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "modeling_chatglm.ChatGLMForConditionalGeneration"
  },
  "bias_dropout_fusion": true,
  "eos_token_id": 2,
  "ffn_hidden_size": 13696,
  "fp32_residual_connection": false,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1e-05,
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_layers": 28,
  "original_rope": true,
  "pad_token_id": 2,
  "padded_vocab_size": 65024,
  "post_layer_norm": true,
  "quantization_bit": 0,
  "rmsnorm": true,
  "seq_length": 32768,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.30.2",
  "use_cache": true
}

[WARNING|configuration_utils.py:549] 2023-06-27 14:44:51,777 >> You are using a model of type chatglm to instantiate a model of type . This is not supported for all configurations of models and can yield errors.
[INFO|modeling_utils.py:2575] 2023-06-27 14:44:51,810 >> loading weights file /home/apps/gzx/LocalModelHub/chatglm2_6b/hf/pytorch_model.bin.index.json
[INFO|configuration_utils.py:577] 2023-06-27 14:44:51,811 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 2,
  "transformers_version": "4.30.2"
}

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:03<00:18,  3.09s/it]Loading checkpoint shards:  14%|█▍        | 1/7 [00:03<00:20,  3.40s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:06<00:15,  3.19s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:06<00:17,  3.45s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:09<00:12,  3.18s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:09<00:13,  3.38s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:13<00:09,  3.31s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:13<00:10,  3.39s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:16<00:06,  3.46s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:17<00:07,  3.53s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:20<00:03,  3.50s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:20<00:03,  3.62s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:21<00:00,  2.79s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:21<00:00,  3.13s/it]
[INFO|modeling_utils.py:3295] 2023-06-27 14:45:13,782 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.

[INFO|modeling_utils.py:3303] 2023-06-27 14:45:13,782 >> All the weights of ChatGLMForConditionalGeneration were initialized from the model checkpoint at /home/apps/gzx/LocalModelHub/chatglm2_6b/hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use ChatGLMForConditionalGeneration for predictions without further training.
[INFO|modeling_utils.py:2927] 2023-06-27 14:45:13,784 >> Generation config file not found, using a generation config created from the model config.
06/27/2023 14:45:13 - INFO - utils.common - Fine-tuning method: LoRA
Loading checkpoint shards: 100%|██████████| 7/7 [00:23<00:00,  3.14s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:23<00:00,  3.30s/it]
06/27/2023 14:45:15 - INFO - utils.common - Fine-tuning method: LoRA
trainable params: 1949696 || all params: 6245533696 || trainable%: 0.0312
Running tokenizer on dataset:   0%|          | 0/5000 [00:00<?, ? examples/s]06/27/2023 14:45:21 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/apps/.cache/huggingface/datasets/json/default-e03ba5b173cf57e1/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-4ef74f89af67c74a.arrow
Running tokenizer on dataset:  20%|██        | 1000/5000 [00:00<00:02, 1413.93 examples/s]Running tokenizer on dataset:  40%|████      | 2000/5000 [00:01<00:02, 1480.25 examples/s]trainable params: 1949696 || all params: 6245533696 || trainable%: 0.0312
Running tokenizer on dataset:  60%|██████    | 3000/5000 [00:02<00:01, 1464.24 examples/s]Running tokenizer on dataset:  80%|████████  | 4000/5000 [00:02<00:00, 1455.98 examples/s]Running tokenizer on dataset: 100%|██████████| 5000/5000 [00:03<00:00, 1452.78 examples/s]                                                                                          input_ids:
[64790, 64792, 790, 30951, 517, 30910, 30939, 30996, 13, 13, 54761, 31211, 31822, 32065, 53015, 31791, 31744, 49635, 34677, 54619, 32941, 54530, 35195, 34677, 31123, 54556, 54236, 41578, 32941, 54530, 32727, 35195, 34677, 31123, 54724, 55155, 54571, 54236, 34677, 32667, 32911, 54887, 31123, 31696, 32363, 32495, 33652, 48084, 31123, 54952, 54744, 33840, 36410, 30966, 30940, 54550, 54952, 13, 296, 31791, 31744, 49635, 34677, 31211, 55163, 55149, 31007, 36053, 31007, 32645, 54931, 31007, 37035, 31007, 38427, 31007, 32033, 54931, 31007, 55430, 57264, 31007, 54940, 56949, 55835, 31007, 55120, 54635, 54931, 31007, 34277, 31007, 39783, 31007, 57212, 55386, 31007, 34454, 54931, 31007, 55120, 54635, 31007, 33408, 54936, 55202, 31007, 31638, 31007, 54620, 54561, 55145, 31007, 58686, 57440, 57440, 31007, 56635, 56048, 55529, 31007, 32330, 31709, 31007, 36463, 31007, 54940, 55608, 31007, 51219, 31007, 40136, 31123, 32941, 54611, 55296, 56007, 30944, 4678, 30961, 54586, 43680, 38682, 54612, 54530, 35195, 34677, 31211, 43399, 31007, 55120, 54635, 54931, 31007, 55163, 55149, 13, 296, 13, 13, 55437, 31211, 30910, 54236, 34677, 31211, 55120, 54635, 54931, 31007, 55163, 55149, 31155, 32495, 33652, 33088, 55296, 56007, 30944, 4678, 30961, 54586, 43680, 38682, 54612, 32104, 43399, 31201, 55120, 54635, 54931, 54609, 33162, 31123, 31738, 31815, 55163, 55149, 31201, 55120, 54635, 54931, 40024, 32540, 31870, 31123, 34285, 32941, 44762, 53208, 32286, 37373, 54542, 31848, 39255, 54629, 32600, 31155, 2]
inputs:
[Round 1]

问：你的任务是根据我的历史偏好标签与影片的附属标签，来推断我喜欢影片的哪些附属标签，并仅用推断标签组成一段话，要求符合推荐理由的风格，字数尽量不超过30个字
    我的历史偏好标签：益智|亲子|娱乐类|喜剧|冒险|游戏类|童谣|张秉君|早教类|动画|宠物|谭笑|辅导类|早教|英文儿歌|生活|合家欢|缪莹莹|贾晨露|语言能力|玩具|张伟|动画片|友情，影片《飞狗MOCO之我爱我家》的附属标签：奇幻|早教类|益智
    

答： 推断标签：早教类|益智。推荐理由：《飞狗MOCO之我爱我家》拥有奇幻、早教类等元素，对于喜欢益智、早教类内容的观众来说，这部影片一定能给您带来乐趣和知识的双重享受。
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30910, 54236, 34677, 31211, 55120, 54635, 54931, 31007, 55163, 55149, 31155, 32495, 33652, 33088, 55296, 56007, 30944, 4678, 30961, 54586, 43680, 38682, 54612, 32104, 43399, 31201, 55120, 54635, 54931, 54609, 33162, 31123, 31738, 31815, 55163, 55149, 31201, 55120, 54635, 54931, 40024, 32540, 31870, 31123, 34285, 32941, 44762, 53208, 32286, 37373, 54542, 31848, 39255, 54629, 32600, 31155, 2]
labels:
推断标签：早教类|益智。推荐理由：《飞狗MOCO之我爱我家》拥有奇幻、早教类等元素，对于喜欢益智、早教类内容的观众来说，这部影片一定能给您带来乐趣和知识的双重享受。
Running tokenizer on dataset:   0%|          | 0/5000 [00:00<?, ? examples/s]Running tokenizer on dataset:  20%|██        | 1000/5000 [00:00<00:02, 1407.66 examples/s]Running tokenizer on dataset:  40%|████      | 2000/5000 [00:01<00:02, 1458.03 examples/s]Running tokenizer on dataset:  60%|██████    | 3000/5000 [00:02<00:01, 1468.90 examples/s]Running tokenizer on dataset:  80%|████████  | 4000/5000 [00:02<00:00, 1456.28 examples/s]Running tokenizer on dataset: 100%|██████████| 5000/5000 [00:03<00:00, 1447.86 examples/s]                                                                                          input_ids:
[64790, 64792, 790, 30951, 517, 30910, 30939, 30996, 13, 13, 54761, 31211, 31822, 32065, 53015, 31791, 31744, 49635, 34677, 54619, 32941, 54530, 35195, 34677, 31123, 54556, 54236, 41578, 32941, 54530, 32727, 35195, 34677, 31123, 54724, 55155, 54571, 54236, 34677, 32667, 32911, 54887, 31123, 31696, 32363, 32495, 33652, 48084, 31123, 54952, 54744, 33840, 36410, 30966, 30940, 54550, 54952, 13, 296, 31791, 31744, 49635, 34677, 31211, 55163, 55149, 31007, 36053, 31007, 32645, 54931, 31007, 37035, 31007, 38427, 31007, 32033, 54931, 31007, 55430, 57264, 31007, 54940, 56949, 55835, 31007, 55120, 54635, 54931, 31007, 34277, 31007, 39783, 31007, 57212, 55386, 31007, 34454, 54931, 31007, 55120, 54635, 31007, 33408, 54936, 55202, 31007, 31638, 31007, 54620, 54561, 55145, 31007, 58686, 57440, 57440, 31007, 56635, 56048, 55529, 31007, 32330, 31709, 31007, 36463, 31007, 54940, 55608, 31007, 51219, 31007, 40136, 31123, 32941, 54611, 55296, 56007, 30944, 4678, 30961, 54586, 43680, 38682, 54612, 54530, 35195, 34677, 31211, 43399, 31007, 55120, 54635, 54931, 31007, 55163, 55149, 13, 296, 13, 13, 55437, 31211, 30910, 54236, 34677, 31211, 55120, 54635, 54931, 31007, 55163, 55149, 31155, 32495, 33652, 33088, 55296, 56007, 30944, 4678, 30961, 54586, 43680, 38682, 54612, 32104, 43399, 31201, 55120, 54635, 54931, 54609, 33162, 31123, 31738, 31815, 55163, 55149, 31201, 55120, 54635, 54931, 40024, 32540, 31870, 31123, 34285, 32941, 44762, 53208, 32286, 37373, 54542, 31848, 39255, 54629, 32600, 31155, 2]
inputs:
[Round 1]

问：你的任务是根据我的历史偏好标签与影片的附属标签，来推断我喜欢影片的哪些附属标签，并仅用推断标签组成一段话，要求符合推荐理由的风格，字数尽量不超过30个字
    我的历史偏好标签：益智|亲子|娱乐类|喜剧|冒险|游戏类|童谣|张秉君|早教类|动画|宠物|谭笑|辅导类|早教|英文儿歌|生活|合家欢|缪莹莹|贾晨露|语言能力|玩具|张伟|动画片|友情，影片《飞狗MOCO之我爱我家》的附属标签：奇幻|早教类|益智
    

答： 推断标签：早教类|益智。推荐理由：《飞狗MOCO之我爱我家》拥有奇幻、早教类等元素，对于喜欢益智、早教类内容的观众来说，这部影片一定能给您带来乐趣和知识的双重享受。
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30910, 54236, 34677, 31211, 55120, 54635, 54931, 31007, 55163, 55149, 31155, 32495, 33652, 33088, 55296, 56007, 30944, 4678, 30961, 54586, 43680, 38682, 54612, 32104, 43399, 31201, 55120, 54635, 54931, 54609, 33162, 31123, 31738, 31815, 55163, 55149, 31201, 55120, 54635, 54931, 40024, 32540, 31870, 31123, 34285, 32941, 44762, 53208, 32286, 37373, 54542, 31848, 39255, 54629, 32600, 31155, 2]
labels:
推断标签：早教类|益智。推荐理由：《飞狗MOCO之我爱我家》拥有奇幻、早教类等元素，对于喜欢益智、早教类内容的观众来说，这部影片一定能给您带来乐趣和知识的双重享受。
06/27/2023 14:45:34 - WARNING - utils.peft_trainer - Previous log file in this folder will be deleted.
[INFO|trainer.py:1786] 2023-06-27 14:45:39,422 >> ***** Running training *****
[INFO|trainer.py:1787] 2023-06-27 14:45:39,423 >>   Num examples = 5,000
[INFO|trainer.py:1788] 2023-06-27 14:45:39,423 >>   Num Epochs = 3
[INFO|trainer.py:1789] 2023-06-27 14:45:39,423 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:1790] 2023-06-27 14:45:39,423 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1791] 2023-06-27 14:45:39,423 >>   Gradient Accumulation steps = 16
[INFO|trainer.py:1792] 2023-06-27 14:45:39,423 >>   Total optimization steps = 468
[INFO|trainer.py:1793] 2023-06-27 14:45:39,425 >>   Number of trainable parameters = 1,949,696
  0%|          | 0/468 [00:00<?, ?it/s]  0%|          | 1/468 [00:06<48:59,  6.30s/it]06/27/2023 14:45:45 - INFO - torch.nn.parallel.distributed - Reducer buckets have been rebuilt in this iteration.
06/27/2023 14:45:45 - INFO - torch.nn.parallel.distributed - Reducer buckets have been rebuilt in this iteration.
  0%|          | 2/468 [00:11<44:22,  5.71s/it]  1%|          | 3/468 [00:16<42:10,  5.44s/it]  1%|          | 4/468 [00:21<41:21,  5.35s/it]  1%|          | 5/468 [00:27<41:06,  5.33s/it]  1%|▏         | 6/468 [00:32<41:07,  5.34s/it]  1%|▏         | 7/468 [00:38<41:21,  5.38s/it]  2%|▏         | 8/468 [00:43<41:02,  5.35s/it]  2%|▏         | 9/468 [00:48<41:17,  5.40s/it]  2%|▏         | 10/468 [00:54<42:19,  5.54s/it]                                                {'loss': 1.4183, 'learning_rate': 0.0009988738792578126, 'epoch': 0.06}
  2%|▏         | 10/468 [00:54<42:19,  5.54s/it]  2%|▏         | 11/468 [01:00<43:05,  5.66s/it]  3%|▎         | 12/468 [01:06<42:40,  5.61s/it]  3%|▎         | 13/468 [01:11<42:54,  5.66s/it]  3%|▎         | 14/468 [01:17<42:47,  5.66s/it]  3%|▎         | 15/468 [01:23<42:45,  5.66s/it]  3%|▎         | 16/468 [01:28<42:39,  5.66s/it]  4%|▎         | 17/468 [01:34<42:41,  5.68s/it]  4%|▍         | 18/468 [01:40<42:57,  5.73s/it]  4%|▍         | 19/468 [01:46<43:07,  5.76s/it]  4%|▍         | 20/468 [01:52<43:03,  5.77s/it]                                                {'loss': 0.8996, 'learning_rate': 0.0009955005896229543, 'epoch': 0.13}
  4%|▍         | 20/468 [01:52<43:03,  5.77s/it]  4%|▍         | 21/468 [01:57<43:14,  5.80s/it]  5%|▍         | 22/468 [02:04<43:43,  5.88s/it]  5%|▍         | 23/468 [02:09<43:48,  5.91s/it]  5%|▌         | 24/468 [02:16<44:09,  5.97s/it]  5%|▌         | 25/468 [02:22<45:13,  6.13s/it]  6%|▌         | 26/468 [02:28<45:05,  6.12s/it]  6%|▌         | 27/468 [02:34<44:53,  6.11s/it]  6%|▌         | 28/468 [02:40<44:21,  6.05s/it]  6%|▌         | 29/468 [02:46<44:11,  6.04s/it]  6%|▋         | 30/468 [02:52<43:23,  5.94s/it]                                                {'loss': 0.8329, 'learning_rate': 0.0009898953260211339, 'epoch': 0.19}
  6%|▋         | 30/468 [02:52<43:23,  5.94s/it]  7%|▋         | 31/468 [02:58<42:54,  5.89s/it]  7%|▋         | 32/468 [03:04<42:41,  5.87s/it]  7%|▋         | 33/468 [03:09<42:23,  5.85s/it]  7%|▋         | 34/468 [03:15<41:39,  5.76s/it]  7%|▋         | 35/468 [03:20<41:09,  5.70s/it]  8%|▊         | 36/468 [03:26<40:52,  5.68s/it]  8%|▊         | 37/468 [03:32<40:46,  5.68s/it]  8%|▊         | 38/468 [03:38<41:13,  5.75s/it]  8%|▊         | 39/468 [03:43<41:09,  5.76s/it]  9%|▊         | 40/468 [03:50<41:52,  5.87s/it]                                                {'loss': 0.8215, 'learning_rate': 0.0009820833372667813, 'epoch': 0.26}
  9%|▊         | 40/468 [03:50<41:52,  5.87s/it]  9%|▉         | 41/468 [03:55<41:42,  5.86s/it]  9%|▉         | 42/468 [04:01<42:01,  5.92s/it]  9%|▉         | 43/468 [04:07<41:56,  5.92s/it]  9%|▉         | 44/468 [04:14<42:22,  6.00s/it] 10%|▉         | 45/468 [04:20<42:32,  6.03s/it] 10%|▉         | 46/468 [04:26<42:40,  6.07s/it] 10%|█         | 47/468 [04:32<42:49,  6.10s/it] 10%|█         | 48/468 [04:38<42:08,  6.02s/it] 10%|█         | 49/468 [04:44<41:36,  5.96s/it] 11%|█         | 50/468 [04:50<41:17,  5.93s/it]                                                {'loss': 0.8205, 'learning_rate': 0.0009720998123301923, 'epoch': 0.32}
 11%|█         | 50/468 [04:50<41:17,  5.93s/it] 11%|█         | 51/468 [04:55<40:57,  5.89s/it] 11%|█         | 52/468 [05:01<40:18,  5.81s/it] 11%|█▏        | 53/468 [05:07<39:42,  5.74s/it] 12%|█▏        | 54/468 [05:12<39:49,  5.77s/it] 12%|█▏        | 55/468 [05:18<39:11,  5.69s/it] 12%|█▏        | 56/468 [05:24<39:11,  5.71s/it] 12%|█▏        | 57/468 [05:29<39:24,  5.75s/it] 12%|█▏        | 58/468 [05:35<38:48,  5.68s/it] 13%|█▎        | 59/468 [05:41<38:31,  5.65s/it] 13%|█▎        | 60/468 [05:46<38:32,  5.67s/it]                                                {'loss': 0.7732, 'learning_rate': 0.0009599897218294122, 'epoch': 0.38}
 13%|█▎        | 60/468 [05:46<38:32,  5.67s/it] 13%|█▎        | 61/468 [05:52<38:45,  5.71s/it] 13%|█▎        | 62/468 [05:58<38:45,  5.73s/it] 13%|█▎        | 63/468 [06:04<38:58,  5.77s/it] 14%|█▎        | 64/468 [06:10<39:25,  5.85s/it] 14%|█▍        | 65/468 [06:16<40:26,  6.02s/it] 14%|█▍        | 66/468 [06:22<40:28,  6.04s/it] 14%|█▍        | 67/468 [06:28<40:35,  6.07s/it] 15%|█▍        | 68/468 [06:34<40:27,  6.07s/it] 15%|█▍        | 69/468 [06:40<40:11,  6.04s/it] 15%|█▍        | 70/468 [06:47<40:04,  6.04s/it]                                                {'loss': 0.7809, 'learning_rate': 0.0009458076154608515, 'epoch': 0.45}
 15%|█▍        | 70/468 [06:47<40:04,  6.04s/it] 15%|█▌        | 71/468 [06:52<39:28,  5.97s/it] 15%|█▌        | 72/468 [06:58<39:30,  5.99s/it] 16%|█▌        | 73/468 [07:04<39:05,  5.94s/it] 16%|█▌        | 74/468 [07:10<38:43,  5.90s/it] 16%|█▌        | 75/468 [07:16<38:39,  5.90s/it] 16%|█▌        | 76/468 [07:22<38:08,  5.84s/it] 16%|█▋        | 77/468 [07:27<37:53,  5.81s/it] 17%|█▋        | 78/468 [07:33<37:46,  5.81s/it] 17%|█▋        | 79/468 [07:39<37:53,  5.84s/it] 17%|█▋        | 80/468 [07:45<37:31,  5.80s/it]                                                {'loss': 0.7511, 'learning_rate': 0.0009296173762811085, 'epoch': 0.51}
 17%|█▋        | 80/468 [07:45<37:31,  5.80s/it] 17%|█▋        | 81/468 [07:51<37:24,  5.80s/it] 18%|█▊        | 82/468 [07:56<37:27,  5.82s/it] 18%|█▊        | 83/468 [08:02<37:17,  5.81s/it] 18%|█▊        | 84/468 [08:08<36:56,  5.77s/it] 18%|█▊        | 85/468 [08:14<37:06,  5.81s/it] 18%|█▊        | 86/468 [08:20<37:24,  5.87s/it] 19%|█▊        | 87/468 [08:26<38:48,  6.11s/it] 19%|█▉        | 88/468 [08:33<38:50,  6.13s/it] 19%|█▉        | 89/468 [08:39<39:10,  6.20s/it] 19%|█▉        | 90/468 [08:45<39:14,  6.23s/it]                                                {'loss': 0.751, 'learning_rate': 0.0009114919329468282, 'epoch': 0.58}
 19%|█▉        | 90/468 [08:45<39:14,  6.23s/it] 19%|█▉        | 91/468 [08:51<38:39,  6.15s/it] 20%|█▉        | 92/468 [08:57<37:49,  6.04s/it] 20%|█▉        | 93/468 [09:03<37:10,  5.95s/it] 20%|██        | 94/468 [09:09<36:38,  5.88s/it] 20%|██        | 95/468 [09:14<35:50,  5.77s/it] 21%|██        | 96/468 [09:20<35:55,  5.80s/it] 21%|██        | 97/468 [09:26<35:45,  5.78s/it] 21%|██        | 98/468 [09:31<35:30,  5.76s/it] 21%|██        | 99/468 [09:37<34:55,  5.68s/it] 21%|██▏       | 100/468 [09:43<35:04,  5.72s/it]                                                 {'loss': 0.7246, 'learning_rate': 0.0008915129312088111, 'epoch': 0.64}
 21%|██▏       | 100/468 [09:43<35:04,  5.72s/it] 22%|██▏       | 101/468 [09:48<35:13,  5.76s/it] 22%|██▏       | 102/468 [09:54<35:02,  5.74s/it] 22%|██▏       | 103/468 [10:00<34:48,  5.72s/it] 22%|██▏       | 104/468 [10:06<34:42,  5.72s/it] 22%|██▏       | 105/468 [10:11<34:36,  5.72s/it] 23%|██▎       | 106/468 [10:17<35:08,  5.82s/it] 23%|██▎       | 107/468 [10:23<35:02,  5.82s/it] 23%|██▎       | 108/468 [10:29<35:05,  5.85s/it] 23%|██▎       | 109/468 [10:35<35:38,  5.96s/it] 24%|██▎       | 110/468 [10:41<35:46,  6.00s/it]                                                 {'loss': 0.7732, 'learning_rate': 0.0008720212520454054, 'epoch': 0.7}
 24%|██▎       | 110/468 [10:41<35:46,  6.00s/it] 24%|██▎       | 111/468 [10:48<35:58,  6.05s/it] 24%|██▍       | 112/468 [10:54<36:19,  6.12s/it] 24%|██▍       | 113/468 [11:00<36:03,  6.09s/it] 24%|██▍       | 114/468 [11:06<35:30,  6.02s/it] 25%|██▍       | 115/468 [11:12<35:10,  5.98s/it] 25%|██▍       | 116/468 [11:17<34:30,  5.88s/it] 25%|██▌       | 117/468 [11:23<34:20,  5.87s/it] 25%|██▌       | 118/468 [11:29<33:43,  5.78s/it] 25%|██▌       | 119/468 [11:34<33:38,  5.78s/it] 26%|██▌       | 120/468 [11:40<33:25,  5.76s/it]                                                 {'loss': 0.7198, 'learning_rate': 0.0008487750003924341, 'epoch': 0.77}
 26%|██▌       | 120/468 [11:40<33:25,  5.76s/it] 26%|██▌       | 121/468 [11:46<32:52,  5.69s/it] 26%|██▌       | 122/468 [11:51<32:54,  5.71s/it] 26%|██▋       | 123/468 [11:57<32:41,  5.68s/it] 26%|██▋       | 124/468 [12:03<32:19,  5.64s/it] 27%|██▋       | 125/468 [12:08<32:17,  5.65s/it] 27%|██▋       | 126/468 [12:14<32:19,  5.67s/it] 27%|██▋       | 127/468 [12:20<32:24,  5.70s/it] 27%|██▋       | 128/468 [12:26<32:38,  5.76s/it] 28%|██▊       | 129/468 [12:32<32:52,  5.82s/it] 28%|██▊       | 130/468 [12:38<32:54,  5.84s/it]                                                 {'loss': 0.7297, 'learning_rate': 0.0008239576976902694, 'epoch': 0.83}
 28%|██▊       | 130/468 [12:38<32:54,  5.84s/it] 28%|██▊       | 131/468 [12:44<33:23,  5.95s/it] 28%|██▊       | 132/468 [12:50<33:35,  6.00s/it] 28%|██▊       | 133/468 [12:56<33:42,  6.04s/it] 29%|██▊       | 134/468 [13:02<33:59,  6.11s/it] 29%|██▉       | 135/468 [13:09<34:16,  6.18s/it] 29%|██▉       | 136/468 [13:14<33:37,  6.08s/it] 29%|██▉       | 137/468 [13:20<33:17,  6.03s/it] 29%|██▉       | 138/468 [13:26<32:48,  5.97s/it] 30%|██▉       | 139/468 [13:32<32:23,  5.91s/it] 30%|██▉       | 140/468 [13:38<31:51,  5.83s/it]                                                 {'loss': 0.6993, 'learning_rate': 0.0007976811330562637, 'epoch': 0.9}
 30%|██▉       | 140/468 [13:38<31:51,  5.83s/it] 30%|███       | 141/468 [13:43<31:38,  5.81s/it] 30%|███       | 142/468 [13:49<31:18,  5.76s/it] 31%|███       | 143/468 [13:55<31:02,  5.73s/it] 31%|███       | 144/468 [14:00<30:43,  5.69s/it] 31%|███       | 145/468 [14:06<30:28,  5.66s/it] 31%|███       | 146/468 [14:11<30:14,  5.64s/it] 31%|███▏      | 147/468 [14:17<29:57,  5.60s/it] 32%|███▏      | 148/468 [14:23<30:05,  5.64s/it] 32%|███▏      | 149/468 [14:29<30:19,  5.70s/it] 32%|███▏      | 150/468 [14:34<30:16,  5.71s/it]                                                 {'loss': 0.693, 'learning_rate': 0.0007700636688282879, 'epoch': 0.96}
 32%|███▏      | 150/468 [14:34<30:16,  5.71s/it] 32%|███▏      | 151/468 [14:40<30:19,  5.74s/it] 32%|███▏      | 152/468 [14:46<30:35,  5.81s/it] 33%|███▎      | 153/468 [14:52<30:54,  5.89s/it] 33%|███▎      | 154/468 [14:58<31:04,  5.94s/it] 33%|███▎      | 155/468 [15:04<31:19,  6.00s/it] 33%|███▎      | 156/468 [15:11<31:34,  6.07s/it] 34%|███▎      | 157/468 [15:17<31:23,  6.06s/it] 34%|███▍      | 158/468 [15:23<31:26,  6.08s/it] 34%|███▍      | 159/468 [15:29<31:05,  6.04s/it] 34%|███▍      | 160/468 [15:35<31:04,  6.05s/it]                                                 {'loss': 0.6848, 'learning_rate': 0.0007412297074035968, 'epoch': 1.02}
 34%|███▍      | 160/468 [15:35<31:04,  6.05s/it] 34%|███▍      | 161/468 [15:41<30:36,  5.98s/it] 35%|███▍      | 162/468 [15:46<30:15,  5.93s/it] 35%|███▍      | 163/468 [15:52<29:38,  5.83s/it] 35%|███▌      | 164/468 [15:58<29:28,  5.82s/it] 35%|███▌      | 165/468 [16:03<29:08,  5.77s/it] 35%|███▌      | 166/468 [16:09<28:53,  5.74s/it] 36%|███▌      | 167/468 [16:15<28:34,  5.69s/it] 36%|███▌      | 168/468 [16:20<28:23,  5.68s/it] 36%|███▌      | 169/468 [16:26<28:22,  5.70s/it] 36%|███▋      | 170/468 [16:32<28:40,  5.77s/it]                                                 {'loss': 0.6253, 'learning_rate': 0.0007113091308703497, 'epoch': 1.09}
 36%|███▋      | 170/468 [16:32<28:40,  5.77s/it] 37%|███▋      | 171/468 [16:38<28:33,  5.77s/it] 37%|███▋      | 172/468 [16:44<28:39,  5.81s/it] 37%|███▋      | 173/468 [16:50<28:49,  5.86s/it] 37%|███▋      | 174/468 [16:56<28:55,  5.90s/it] 37%|███▋      | 175/468 [17:02<29:07,  5.97s/it] 38%|███▊      | 176/468 [17:08<29:08,  5.99s/it] 38%|███▊      | 177/468 [17:14<29:01,  5.99s/it] 38%|███▊      | 178/468 [17:20<29:15,  6.05s/it] 38%|███▊      | 179/468 [17:26<28:59,  6.02s/it] 38%|███▊      | 180/468 [17:32<28:26,  5.93s/it]                                                 {'loss': 0.6565, 'learning_rate': 0.0006804367159559562, 'epoch': 1.15}
 38%|███▊      | 180/468 [17:32<28:26,  5.93s/it] 39%|███▊      | 181/468 [17:37<28:13,  5.90s/it] 39%|███▉      | 182/468 [17:43<27:44,  5.82s/it] 39%|███▉      | 183/468 [17:49<27:49,  5.86s/it] 39%|███▉      | 184/468 [17:55<27:23,  5.79s/it] 40%|███▉      | 185/468 [18:01<27:25,  5.82s/it] 40%|███▉      | 186/468 [18:06<27:01,  5.75s/it] 40%|███▉      | 187/468 [18:12<26:55,  5.75s/it] 40%|████      | 188/468 [18:18<26:54,  5.77s/it] 40%|████      | 189/468 [18:23<26:42,  5.74s/it] 41%|████      | 190/468 [18:29<26:49,  5.79s/it]                                                 {'loss': 0.6055, 'learning_rate': 0.0006487515269276015, 'epoch': 1.22}
 41%|████      | 190/468 [18:29<26:49,  5.79s/it] 41%|████      | 191/468 [18:35<26:50,  5.81s/it] 41%|████      | 192/468 [18:41<26:43,  5.81s/it] 41%|████      | 193/468 [18:47<26:41,  5.82s/it] 41%|████▏     | 194/468 [18:53<26:31,  5.81s/it] 42%|████▏     | 195/468 [18:59<26:45,  5.88s/it] 42%|████▏     | 196/468 [19:05<27:18,  6.02s/it] 42%|████▏     | 197/468 [19:11<27:11,  6.02s/it] 42%|████▏     | 198/468 [19:17<27:08,  6.03s/it] 43%|████▎     | 199/468 [19:23<27:20,  6.10s/it] 43%|████▎     | 200/468 [19:29<26:59,  6.04s/it]                                                 {'loss': 0.6036, 'learning_rate': 0.0006163962891796261, 'epoch': 1.28}
 43%|████▎     | 200/468 [19:29<26:59,  6.04s/it] 43%|████▎     | 201/468 [19:35<26:29,  5.95s/it] 43%|████▎     | 202/468 [19:41<26:27,  5.97s/it] 43%|████▎     | 203/468 [19:47<26:42,  6.05s/it] 44%|████▎     | 204/468 [19:53<26:07,  5.94s/it] 44%|████▍     | 205/468 [19:58<25:31,  5.82s/it] 44%|████▍     | 206/468 [20:04<25:11,  5.77s/it] 44%|████▍     | 207/468 [20:10<24:46,  5.70s/it] 44%|████▍     | 208/468 [20:15<24:38,  5.69s/it] 45%|████▍     | 209/468 [20:21<24:21,  5.64s/it] 45%|████▍     | 210/468 [20:26<24:16,  5.65s/it]                                                 {'loss': 0.6216, 'learning_rate': 0.0005835167463294154, 'epoch': 1.34}
 45%|████▍     | 210/468 [20:26<24:16,  5.65s/it] 45%|████▌     | 211/468 [20:32<24:12,  5.65s/it] 45%|████▌     | 212/468 [20:38<24:21,  5.71s/it] 46%|████▌     | 213/468 [20:44<24:30,  5.76s/it] 46%|████▌     | 214/468 [20:50<24:25,  5.77s/it] 46%|████▌     | 215/468 [20:56<24:26,  5.80s/it] 46%|████▌     | 216/468 [21:02<24:44,  5.89s/it] 46%|████▋     | 217/468 [21:08<24:50,  5.94s/it] 47%|████▋     | 218/468 [21:14<24:59,  6.00s/it] 47%|████▋     | 219/468 [21:20<24:56,  6.01s/it] 47%|████▋     | 220/468 [21:26<24:59,  6.05s/it]                                                 {'loss': 0.618, 'learning_rate': 0.0005502610037177585, 'epoch': 1.41}
 47%|████▋     | 220/468 [21:26<24:59,  6.05s/it] 47%|████▋     | 221/468 [21:32<24:57,  6.06s/it] 47%|████▋     | 222/468 [21:38<24:40,  6.02s/it] 48%|████▊     | 223/468 [21:44<24:45,  6.06s/it] 48%|████▊     | 224/468 [21:50<24:13,  5.96s/it] 48%|████▊     | 225/468 [21:56<23:50,  5.89s/it] 48%|████▊     | 226/468 [22:01<23:30,  5.83s/it] 49%|████▊     | 227/468 [22:07<23:26,  5.84s/it] 49%|████▊     | 228/468 [22:13<22:59,  5.75s/it] 49%|████▉     | 229/468 [22:18<22:48,  5.72s/it] 49%|████▉     | 230/468 [22:24<22:31,  5.68s/it]                                                 {'loss': 0.6084, 'learning_rate': 0.0005167788612708628, 'epoch': 1.47}
 49%|████▉     | 230/468 [22:24<22:31,  5.68s/it] 49%|████▉     | 231/468 [22:30<22:20,  5.66s/it] 50%|████▉     | 232/468 [22:35<22:33,  5.74s/it] 50%|████▉     | 233/468 [22:42<22:50,  5.83s/it] 50%|█████     | 234/468 [22:48<23:02,  5.91s/it] 50%|█████     | 235/468 [22:53<22:50,  5.88s/it] 50%|█████     | 236/468 [23:00<23:05,  5.97s/it] 51%|█████     | 237/468 [23:06<23:15,  6.04s/it] 51%|█████     | 238/468 [23:12<23:07,  6.03s/it] 51%|█████     | 239/468 [23:18<23:09,  6.07s/it] 51%|█████▏    | 240/468 [23:24<23:14,  6.12s/it]                                                 {'loss': 0.627, 'learning_rate': 0.00048322113872913743, 'epoch': 1.54}
 51%|█████▏    | 240/468 [23:24<23:14,  6.12s/it] 51%|█████▏    | 241/468 [23:30<23:09,  6.12s/it] 52%|█████▏    | 242/468 [23:36<22:49,  6.06s/it] 52%|█████▏    | 243/468 [23:42<22:44,  6.06s/it] 52%|█████▏    | 244/468 [23:48<22:18,  5.98s/it] 52%|█████▏    | 245/468 [23:54<21:50,  5.88s/it] 53%|█████▎    | 246/468 [23:59<21:34,  5.83s/it] 53%|█████▎    | 247/468 [24:05<21:20,  5.80s/it] 53%|█████▎    | 248/468 [24:11<21:05,  5.75s/it] 53%|█████▎    | 249/468 [24:16<20:45,  5.69s/it] 53%|█████▎    | 250/468 [24:22<20:29,  5.64s/it]                                                 {'loss': 0.6004, 'learning_rate': 0.00044973899628224153, 'epoch': 1.6}
 53%|█████▎    | 250/468 [24:22<20:29,  5.64s/it] 54%|█████▎    | 251/468 [24:28<20:31,  5.68s/it] 54%|█████▍    | 252/468 [24:33<20:36,  5.73s/it] 54%|█████▍    | 253/468 [24:39<20:25,  5.70s/it] 54%|█████▍    | 254/468 [24:45<20:07,  5.64s/it] 54%|█████▍    | 255/468 [24:50<20:05,  5.66s/it] 55%|█████▍    | 256/468 [24:56<19:52,  5.63s/it] 55%|█████▍    | 257/468 [25:02<19:54,  5.66s/it] 55%|█████▌    | 258/468 [25:07<19:34,  5.59s/it] 55%|█████▌    | 259/468 [25:13<19:34,  5.62s/it] 56%|█████▌    | 260/468 [25:18<19:33,  5.64s/it]                                                 {'loss': 0.5746, 'learning_rate': 0.0004164832536705845, 'epoch': 1.66}
 56%|█████▌    | 260/468 [25:18<19:33,  5.64s/it] 56%|█████▌    | 261/468 [25:24<19:43,  5.72s/it] 56%|█████▌    | 262/468 [25:31<20:09,  5.87s/it] 56%|█████▌    | 263/468 [25:37<20:07,  5.89s/it] 56%|█████▋    | 264/468 [25:43<20:11,  5.94s/it] 57%|█████▋    | 265/468 [25:49<20:11,  5.97s/it] 57%|█████▋    | 266/468 [25:55<20:05,  5.97s/it] 57%|█████▋    | 267/468 [26:00<19:52,  5.93s/it] 57%|█████▋    | 268/468 [26:06<19:39,  5.90s/it] 57%|█████▋    | 269/468 [26:12<19:41,  5.94s/it] 58%|█████▊    | 270/468 [26:18<19:34,  5.93s/it]                                                 {'loss': 0.5929, 'learning_rate': 0.0003836037108203739, 'epoch': 1.73}
 58%|█████▊    | 270/468 [26:18<19:34,  5.93s/it] 58%|█████▊    | 271/468 [26:24<19:16,  5.87s/it] 58%|█████▊    | 272/468 [26:30<19:00,  5.82s/it] 58%|█████▊    | 273/468 [26:35<18:50,  5.80s/it] 59%|█████▊    | 274/468 [26:41<18:31,  5.73s/it] 59%|█████▉    | 275/468 [26:47<18:18,  5.69s/it] 59%|█████▉    | 276/468 [26:52<18:22,  5.74s/it] 59%|█████▉    | 277/468 [26:58<18:09,  5.70s/it] 59%|█████▉    | 278/468 [27:04<18:04,  5.71s/it] 60%|█████▉    | 279/468 [27:09<17:44,  5.63s/it] 60%|█████▉    | 280/468 [27:15<17:43,  5.66s/it]                                                 {'loss': 0.5934, 'learning_rate': 0.0003512484730723986, 'epoch': 1.79}
 60%|█████▉    | 280/468 [27:15<17:43,  5.66s/it] 60%|██████    | 281/468 [27:21<17:39,  5.66s/it] 60%|██████    | 282/468 [27:26<17:46,  5.73s/it] 60%|██████    | 283/468 [27:32<17:47,  5.77s/it] 61%|██████    | 284/468 [27:38<17:54,  5.84s/it] 61%|██████    | 285/468 [27:44<17:52,  5.86s/it] 61%|██████    | 286/468 [27:51<18:12,  6.00s/it] 61%|██████▏   | 287/468 [27:57<18:16,  6.06s/it] 62%|██████▏   | 288/468 [28:03<18:13,  6.08s/it] 62%|██████▏   | 289/468 [28:09<18:01,  6.04s/it] 62%|██████▏   | 290/468 [28:15<17:55,  6.04s/it]                                                 {'loss': 0.6033, 'learning_rate': 0.000319563284044044, 'epoch': 1.86}
 62%|██████▏   | 290/468 [28:15<17:55,  6.04s/it] 62%|██████▏   | 291/468 [28:21<17:39,  5.99s/it] 62%|██████▏   | 292/468 [28:27<17:29,  5.96s/it] 63%|██████▎   | 293/468 [28:32<17:12,  5.90s/it] 63%|██████▎   | 294/468 [28:38<16:57,  5.85s/it] 63%|██████▎   | 295/468 [28:44<17:02,  5.91s/it] 63%|██████▎   | 296/468 [28:50<16:47,  5.86s/it] 63%|██████▎   | 297/468 [28:56<16:41,  5.86s/it] 64%|██████▎   | 298/468 [29:02<16:29,  5.82s/it] 64%|██████▍   | 299/468 [29:07<16:21,  5.81s/it] 64%|██████▍   | 300/468 [29:13<16:25,  5.87s/it]                                                 {'loss': 0.6052, 'learning_rate': 0.0002886908691296504, 'epoch': 1.92}
 64%|██████▍   | 300/468 [29:13<16:25,  5.87s/it] 64%|██████▍   | 301/468 [29:19<16:23,  5.89s/it] 65%|██████▍   | 302/468 [29:25<16:29,  5.96s/it] 65%|██████▍   | 303/468 [29:32<16:32,  6.01s/it] 65%|██████▍   | 304/468 [29:38<16:45,  6.13s/it] 65%|██████▌   | 305/468 [29:44<16:39,  6.13s/it] 65%|██████▌   | 306/468 [29:50<16:29,  6.11s/it] 66%|██████▌   | 307/468 [29:56<16:23,  6.11s/it] 66%|██████▌   | 308/468 [30:02<16:22,  6.14s/it] 66%|██████▌   | 309/468 [30:09<16:14,  6.13s/it] 66%|██████▌   | 310/468 [30:15<16:16,  6.18s/it]                                                 {'loss': 0.5856, 'learning_rate': 0.0002587702925964034, 'epoch': 1.98}
 66%|██████▌   | 310/468 [30:15<16:16,  6.18s/it] 66%|██████▋   | 311/468 [30:21<15:54,  6.08s/it] 67%|██████▋   | 312/468 [30:27<15:39,  6.02s/it] 67%|██████▋   | 313/468 [30:32<15:20,  5.94s/it] 67%|██████▋   | 314/468 [30:38<15:11,  5.92s/it] 67%|██████▋   | 315/468 [30:44<14:43,  5.78s/it] 68%|██████▊   | 316/468 [30:49<14:30,  5.73s/it] 68%|██████▊   | 317/468 [30:55<14:27,  5.75s/it] 68%|██████▊   | 318/468 [31:01<14:22,  5.75s/it] 68%|██████▊   | 319/468 [31:07<14:22,  5.79s/it] 68%|██████▊   | 320/468 [31:12<14:08,  5.73s/it]                                                 {'loss': 0.5376, 'learning_rate': 0.00022993633117171242, 'epoch': 2.05}
 68%|██████▊   | 320/468 [31:12<14:08,  5.73s/it] 69%|██████▊   | 321/468 [31:18<13:54,  5.68s/it] 69%|██████▉   | 322/468 [31:24<13:53,  5.71s/it] 69%|██████▉   | 323/468 [31:29<13:42,  5.67s/it] 69%|██████▉   | 324/468 [31:35<13:50,  5.76s/it] 69%|██████▉   | 325/468 [31:41<13:42,  5.75s/it] 70%|██████▉   | 326/468 [31:47<13:34,  5.73s/it] 70%|██████▉   | 327/468 [31:52<13:33,  5.77s/it] 70%|███████   | 328/468 [31:58<13:40,  5.86s/it] 70%|███████   | 329/468 [32:04<13:38,  5.89s/it] 71%|███████   | 330/468 [32:10<13:35,  5.91s/it]                                                 {'loss': 0.4913, 'learning_rate': 0.00020231886694373652, 'epoch': 2.11}
 71%|███████   | 330/468 [32:10<13:35,  5.91s/it] 71%|███████   | 331/468 [32:17<13:38,  5.97s/it] 71%|███████   | 332/468 [32:23<13:39,  6.03s/it] 71%|███████   | 333/468 [32:29<13:35,  6.04s/it] 71%|███████▏  | 334/468 [32:35<13:33,  6.07s/it] 72%|███████▏  | 335/468 [32:41<13:13,  5.97s/it] 72%|███████▏  | 336/468 [32:46<13:04,  5.94s/it] 72%|███████▏  | 337/468 [32:53<13:04,  5.99s/it] 72%|███████▏  | 338/468 [32:59<12:57,  5.98s/it] 72%|███████▏  | 339/468 [33:04<12:46,  5.94s/it] 73%|███████▎  | 340/468 [33:10<12:45,  5.98s/it]                                                 {'loss': 0.4981, 'learning_rate': 0.00017604230230973067, 'epoch': 2.18}
 73%|███████▎  | 340/468 [33:10<12:45,  5.98s/it] 73%|███████▎  | 341/468 [33:16<12:31,  5.92s/it] 73%|███████▎  | 342/468 [33:22<12:23,  5.90s/it] 73%|███████▎  | 343/468 [33:28<12:16,  5.89s/it] 74%|███████▎  | 344/468 [33:34<12:02,  5.83s/it] 74%|███████▎  | 345/468 [33:39<11:52,  5.79s/it] 74%|███████▍  | 346/468 [33:45<11:54,  5.86s/it] 74%|███████▍  | 347/468 [33:51<11:51,  5.88s/it] 74%|███████▍  | 348/468 [33:57<11:42,  5.85s/it] 75%|███████▍  | 349/468 [34:03<11:41,  5.89s/it] 75%|███████▍  | 350/468 [34:09<11:38,  5.92s/it]                                                 {'loss': 0.4809, 'learning_rate': 0.00015122499960756603, 'epoch': 2.24}
 75%|███████▍  | 350/468 [34:09<11:38,  5.92s/it] 75%|███████▌  | 351/468 [34:15<11:39,  5.98s/it] 75%|███████▌  | 352/468 [34:21<11:36,  6.00s/it] 75%|███████▌  | 353/468 [34:27<11:37,  6.06s/it] 76%|███████▌  | 354/468 [34:34<11:35,  6.10s/it] 76%|███████▌  | 355/468 [34:40<11:31,  6.12s/it] 76%|███████▌  | 356/468 [34:46<11:29,  6.15s/it] 76%|███████▋  | 357/468 [34:52<11:27,  6.19s/it] 76%|███████▋  | 358/468 [34:58<11:09,  6.09s/it] 77%|███████▋  | 359/468 [35:04<10:48,  5.95s/it] 77%|███████▋  | 360/468 [35:10<10:45,  5.98s/it]                                                 {'loss': 0.5033, 'learning_rate': 0.00012797874795459464, 'epoch': 2.3}
 77%|███████▋  | 360/468 [35:10<10:45,  5.98s/it] 77%|███████▋  | 361/468 [35:16<10:30,  5.89s/it] 77%|███████▋  | 362/468 [35:21<10:17,  5.83s/it] 78%|███████▊  | 363/468 [35:27<10:14,  5.86s/it] 78%|███████▊  | 364/468 [35:33<10:10,  5.87s/it] 78%|███████▊  | 365/468 [35:39<10:01,  5.84s/it] 78%|███████▊  | 366/468 [35:44<09:50,  5.79s/it] 78%|███████▊  | 367/468 [35:50<09:42,  5.76s/it] 79%|███████▊  | 368/468 [35:56<09:40,  5.80s/it] 79%|███████▉  | 369/468 [36:02<09:46,  5.92s/it] 79%|███████▉  | 370/468 [36:08<09:40,  5.93s/it]                                                 {'loss': 0.4847, 'learning_rate': 0.00010640825969547497, 'epoch': 2.37}
 79%|███████▉  | 370/468 [36:08<09:40,  5.93s/it] 79%|███████▉  | 371/468 [36:14<09:39,  5.97s/it] 79%|███████▉  | 372/468 [36:21<09:43,  6.08s/it] 80%|███████▉  | 373/468 [36:27<09:42,  6.13s/it] 80%|███████▉  | 374/468 [36:33<09:34,  6.11s/it] 80%|████████  | 375/468 [36:39<09:25,  6.08s/it] 80%|████████  | 376/468 [36:45<09:22,  6.12s/it] 81%|████████  | 377/468 [36:51<09:09,  6.04s/it] 81%|████████  | 378/468 [36:57<09:04,  6.05s/it] 81%|████████  | 379/468 [37:03<08:56,  6.03s/it] 81%|████████  | 380/468 [37:09<08:41,  5.93s/it]                                                 {'loss': 0.5102, 'learning_rate': 8.661069872719745e-05, 'epoch': 2.43}
 81%|████████  | 380/468 [37:09<08:41,  5.93s/it] 81%|████████▏ | 381/468 [37:15<08:33,  5.90s/it] 82%|████████▏ | 382/468 [37:20<08:24,  5.87s/it] 82%|████████▏ | 383/468 [37:26<08:17,  5.85s/it] 82%|████████▏ | 384/468 [37:32<08:16,  5.91s/it] 82%|████████▏ | 385/468 [37:38<08:06,  5.86s/it] 82%|████████▏ | 386/468 [37:44<07:55,  5.80s/it] 83%|████████▎ | 387/468 [37:50<07:52,  5.84s/it] 83%|████████▎ | 388/468 [37:55<07:45,  5.82s/it] 83%|████████▎ | 389/468 [38:01<07:41,  5.85s/it] 83%|████████▎ | 390/468 [38:07<07:41,  5.92s/it]                                                 {'loss': 0.494, 'learning_rate': 6.867524282596654e-05, 'epoch': 2.5}
 83%|████████▎ | 390/468 [38:07<07:41,  5.92s/it] 84%|████████▎ | 391/468 [38:14<07:45,  6.04s/it] 84%|████████▍ | 392/468 [38:20<07:44,  6.11s/it] 84%|████████▍ | 393/468 [38:26<07:36,  6.09s/it] 84%|████████▍ | 394/468 [38:32<07:30,  6.08s/it] 84%|████████▍ | 395/468 [38:38<07:25,  6.10s/it] 85%|████████▍ | 396/468 [38:44<07:17,  6.08s/it] 85%|████████▍ | 397/468 [38:50<07:10,  6.06s/it] 85%|████████▌ | 398/468 [38:56<06:59,  5.99s/it] 85%|████████▌ | 399/468 [39:02<06:48,  5.92s/it] 85%|████████▌ | 400/468 [39:08<06:38,  5.86s/it]                                                 {'loss': 0.4947, 'learning_rate': 5.268268194742637e-05, 'epoch': 2.56}
 85%|████████▌ | 400/468 [39:08<06:38,  5.86s/it] 86%|████████▌ | 401/468 [39:14<06:36,  5.93s/it] 86%|████████▌ | 402/468 [39:20<06:35,  5.99s/it] 86%|████████▌ | 403/468 [39:26<06:25,  5.93s/it] 86%|████████▋ | 404/468 [39:32<06:21,  5.97s/it] 87%|████████▋ | 405/468 [39:37<06:09,  5.86s/it] 87%|████████▋ | 406/468 [39:43<06:04,  5.88s/it] 87%|████████▋ | 407/468 [39:49<05:56,  5.84s/it] 87%|████████▋ | 408/468 [39:55<05:48,  5.80s/it] 87%|████████▋ | 409/468 [40:00<05:44,  5.83s/it] 88%|████████▊ | 410/468 [40:07<05:43,  5.92s/it]                                                 {'loss': 0.4981, 'learning_rate': 3.8705054309680684e-05, 'epoch': 2.62}
 88%|████████▊ | 410/468 [40:07<05:43,  5.92s/it] 88%|████████▊ | 411/468 [40:12<05:35,  5.89s/it] 88%|████████▊ | 412/468 [40:19<05:34,  5.97s/it] 88%|████████▊ | 413/468 [40:24<05:26,  5.94s/it] 88%|████████▊ | 414/468 [40:30<05:19,  5.92s/it] 89%|████████▊ | 415/468 [40:36<05:16,  5.97s/it] 89%|████████▉ | 416/468 [40:43<05:12,  6.02s/it] 89%|████████▉ | 417/468 [40:49<05:10,  6.10s/it] 89%|████████▉ | 418/468 [40:55<05:04,  6.09s/it] 90%|████████▉ | 419/468 [41:01<04:54,  6.01s/it] 90%|████████▉ | 420/468 [41:07<04:48,  6.01s/it]                                                 {'loss': 0.4833, 'learning_rate': 2.6805321898367318e-05, 'epoch': 2.69}
 90%|████████▉ | 420/468 [41:07<04:48,  6.01s/it] 90%|████████▉ | 421/468 [41:12<04:38,  5.94s/it] 90%|█████████ | 422/468 [41:18<04:28,  5.83s/it] 90%|█████████ | 423/468 [41:24<04:21,  5.81s/it] 91%|█████████ | 424/468 [41:30<04:14,  5.79s/it] 91%|█████████ | 425/468 [41:35<04:09,  5.81s/it] 91%|█████████ | 426/468 [41:41<04:02,  5.78s/it] 91%|█████████ | 427/468 [41:47<03:55,  5.74s/it] 91%|█████████▏| 428/468 [41:53<03:49,  5.73s/it] 92%|█████████▏| 429/468 [41:58<03:44,  5.75s/it] 92%|█████████▏| 430/468 [42:04<03:41,  5.84s/it]                                                 {'loss': 0.5016, 'learning_rate': 1.70370868554659e-05, 'epoch': 2.75}
 92%|█████████▏| 430/468 [42:04<03:41,  5.84s/it] 92%|█████████▏| 431/468 [42:10<03:37,  5.89s/it] 92%|█████████▏| 432/468 [42:16<03:33,  5.93s/it] 93%|█████████▎| 433/468 [42:22<03:29,  5.98s/it] 93%|█████████▎| 434/468 [42:29<03:24,  6.02s/it] 93%|█████████▎| 435/468 [42:35<03:20,  6.06s/it] 93%|█████████▎| 436/468 [42:41<03:16,  6.13s/it] 93%|█████████▎| 437/468 [42:47<03:08,  6.09s/it] 94%|█████████▎| 438/468 [42:53<02:59,  6.00s/it] 94%|█████████▍| 439/468 [42:59<02:53,  5.97s/it] 94%|█████████▍| 440/468 [43:04<02:45,  5.91s/it]                                                 {'loss': 0.4732, 'learning_rate': 9.44435002936167e-06, 'epoch': 2.82}
 94%|█████████▍| 440/468 [43:04<02:45,  5.91s/it] 94%|█████████▍| 441/468 [43:11<02:41,  5.98s/it] 94%|█████████▍| 442/468 [43:16<02:33,  5.90s/it] 95%|█████████▍| 443/468 [43:22<02:25,  5.82s/it] 95%|█████████▍| 444/468 [43:28<02:18,  5.76s/it] 95%|█████████▌| 445/468 [43:33<02:11,  5.73s/it] 95%|█████████▌| 446/468 [43:39<02:05,  5.73s/it] 96%|█████████▌| 447/468 [43:45<02:00,  5.73s/it] 96%|█████████▌| 448/468 [43:50<01:54,  5.75s/it] 96%|█████████▌| 449/468 [43:56<01:50,  5.83s/it] 96%|█████████▌| 450/468 [44:02<01:44,  5.82s/it]                                                 {'loss': 0.4883, 'learning_rate': 4.06131277377414e-06, 'epoch': 2.88}
 96%|█████████▌| 450/468 [44:02<01:44,  5.82s/it] 96%|█████████▋| 451/468 [44:08<01:39,  5.87s/it] 97%|█████████▋| 452/468 [44:15<01:35,  5.97s/it] 97%|█████████▋| 453/468 [44:21<01:29,  5.99s/it] 97%|█████████▋| 454/468 [44:27<01:24,  6.06s/it] 97%|█████████▋| 455/468 [44:33<01:19,  6.13s/it] 97%|█████████▋| 456/468 [44:39<01:12,  6.07s/it] 98%|█████████▊| 457/468 [44:45<01:06,  6.06s/it] 98%|█████████▊| 458/468 [44:51<01:00,  6.00s/it] 98%|█████████▊| 459/468 [44:57<00:53,  5.94s/it] 98%|█████████▊| 460/468 [45:03<00:47,  5.94s/it]                                                 {'loss': 0.4866, 'learning_rate': 9.12222888341252e-07, 'epoch': 2.94}
 98%|█████████▊| 460/468 [45:03<00:47,  5.94s/it] 99%|█████████▊| 461/468 [45:09<00:41,  5.94s/it] 99%|█████████▊| 462/468 [45:15<00:35,  5.95s/it] 99%|█████████▉| 463/468 [45:20<00:29,  5.94s/it] 99%|█████████▉| 464/468 [45:26<00:23,  5.86s/it] 99%|█████████▉| 465/468 [45:32<00:17,  5.79s/it]100%|█████████▉| 466/468 [45:38<00:11,  5.83s/it]100%|█████████▉| 467/468 [45:43<00:05,  5.83s/it]100%|██████████| 468/468 [45:50<00:00,  5.89s/it][INFO|trainer.py:2053] 2023-06-27 15:31:29,545 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 2750.1202, 'train_samples_per_second': 5.454, 'train_steps_per_second': 0.17, 'train_loss': 0.636562021369608, 'epoch': 3.0}
100%|██████████| 468/468 [45:50<00:00,  5.89s/it]100%|██████████| 468/468 [45:50<00:00,  5.88s/it]
***** train metrics *****
  epoch                    =        3.0
  train_loss               =     0.6366
  train_runtime            = 0:45:50.12
  train_samples_per_second =      5.454
  train_steps_per_second   =       0.17
06/27/2023 15:31:29 - INFO - utils.peft_trainer - Saving model checkpoint to ./model/rec_reason_v1_chatglm26b_ckp
